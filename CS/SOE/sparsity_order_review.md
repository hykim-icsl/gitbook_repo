

[l1](#ref1)

r it may not provide a linaerly independent sparse representa-tion as the coefficients do not change unless the source appliessome modulation.From this discussion it becomes clear that MMV techniquesare not applicable to the SMV setting. Another way to interpretthe proposed framework is that by applying Khatri-Rao struc-tured measurement matrices, we can transform the challengingSMV case into a virtual MMV case where then rank estimationschemes can be applied, similar to a MMV setting.Moreover, knowing the sparsity order allows to improve re-construction algorithms by tuning algorithm-specific parameterssuch as the regularization parameterλin LASSO-type tech-niques (which is related toK[14]) or stopping criteria in greedytechniques such as the Orthogonal Matching Pursuit [15].A. Related WorkDue to the prominent role the sparsity order plays in sparsesignal recovery, the lack of knowledge of the sparsity orderhas been recognized as a fundamental gap between theory andpractice [16], [17]. Still, existing literature on this subject isquite scarce and very recent. Early papers on this subject haveproposed to employ sequential measurements [17] and cross-validation type techniques [16], [18] where sequential recon-structions of the signal are considered. Similarly, [19] showsthat the sparsity order can be estimated from the reconstruc-tion, stating bounds on the number of measurements that arerequired for this step. However, the bounds are only found nu-merically, and the reconstruction process involves cumbersomeoptimization problems. As the following results show, this canbe avoided by estimating the sparsity order directly based onthe compressed observations.A different approach is taken in [20], where the authors showthat a specifically tailored measurement procedure which con-sists of a Cauchy and a Gaussian distributed measurement matrixallows to estimate a continuous measure of sparsity given by theratio of squared one- and two-norm of the signal. However, thismeasure is not equal to the sparsity order. In fact, it is continuousand hence needs to be rounded to an integer number (which is notdiscussed in [20]). Moreover, the measurement process is veryrestrictive since the distribution of the measurement matricesis pre-specified. Finally, according to the authors one parame-ter of these distributions should be chosen with respect to thenoise variance, which might be unknown, whereas the resultspresented here do not require the noise’s variance to be known.The authors in [21] propose to use sparse sensing matricessince these allow to infer the degree of sparsity of a signal fromthe degree of sparsity of the measurement. The resulting estima-tor has a very low complexity. However, it is only approachingthe true sparsity in the large system limit and hence not appli-cable to lower-dimensional problems. Moreover, the proposedmeasurement matrices incur a certain performance degradationat the reconstruction stage due to the somewhat higher coher-ence, which is imposed by the restriction to sparse matrices.A link between sparsity order estimation and rank estimationwas put forward in [10] for the multiple measurement vector(MMV) problem, which has also been studied in the authorsown prior work, both for the stationary case [11], [12] as wellas the case of time-varying support for block-stationary signals[13]. However, these approaches require a certain stationarityin the support patterns as well as a temporal variation in thecoefficients of the sparse representation to create linearly in-dependent observations. This limits their applicability in manypractical problems.B. ContributionsThis paper introduces a method for estimating the sparsityorder of a signal from a single snapshot of the compressivemeasurement. In particular, rearrangements of the observationvector into a matrix are considered and is it shown under whichconditions the rank of this matrix coincides with the sparsityorder of the unknown signal. Thereby, the sparsity order canbe estimated by applying any known rank estimation scheme[22]. Since there exist many efficient algorithms which estimatethe model order in presence of perturbations such as noise, thisapproach allows us to handle noisy measurements as well as thecase of approximate sparsity as well.The proposed approach only requires the measurement ma-trix to possess a Khatri-Rao structure, which leaves considerableroom for optimizing their choice. Moreover, in the case of over-lapping blocks, one of the factors needs to be a Vandermondematrix. Therefore, the second part of the manuscript discussesthe design of measurement matrices in presence of these struc-tural constraints. To facilitate the sparsity order estimation aswell as the sparse reconstruction, the factors need to possessa low coherence. Moreover, in the presence of a Khatri-Raostructure, it is best to optimize the factors of the Khatri-Raoproduct independently. Therefore investigate the coherence ofVandermonde matrices is investigated and a new design algo-rithm is proposed, which efficiently constructs Vandermondematrices with low coherence. Additionally, simple upper andlower bounds for the resulting coherence of this algorithm arederived.Compared to the earlier conference version [23], this papercontains more rigorous statements of the theorems on sparsityorder estimation, which contained some ambiguities in theiroriginal formulations, while we also significantly improved thepresentation of the proofs (which had to be partially omitted inthe conference version due to lack of space). Moreover, the op-timization of the measurement matrix with respect to the struc-tural constraints and discussion thereof add to the contributionof the earlier conference version; especially the Vandermondestructure was analyzed very carefully to derive an algorithm forconstructing these as well as coherence bounds for its output. Fi-nally the numerical simulations were improved upon by addinga comparison to exsting methods as well as a more detailedexamination of a more realistic compressed sensing scenario.C. NotationCapital bold face letters denote matrices, whereas lowercaseboldface letters stand for vectors. The abbreviation[N]forN∈Nis used as a shorthand for the set{1,...,N}.Thesetsupp


#ref1
A.Mousavi, A.Maleki, and R.G.Baraniuk, “Asymptotic analysis of LASSOs solution path with implications for approximate message passing,” Statistics, pp.3-4, Sep.2013.